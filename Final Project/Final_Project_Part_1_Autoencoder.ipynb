{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "y_Dn9rzyiqxi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGLKx-6qiz-_"
   },
   "source": [
    "Upload the labels.csv and processed_counts.csv files to colab or your local workspace.\n",
    "\n",
    "This data associates a cell barcode, such as \"AAAGCCTGGCTAAC-1\", to a certain cell type label, such as \"CD14+ Monocyte\". For each cell barcode, there are also log RNA seq counts of 765 different genes, such as HES4.\n",
    "\n",
    "label.csv stores the association between a cell barcode and a cell type label.\n",
    "\n",
    "processed_counts.csv stores the normalized log read counts for each cell, where each row represents a single cell, and each column represents a gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WelsjSzviy4m"
   },
   "outputs": [],
   "source": [
    "labels_pd = pd.read_csv(\"labels.csv\")\n",
    "counts_pd = pd.read_csv(\"processed_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "aIX8kcTXi7EV"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def shuffle_data(x, y):\n",
    "    X = pd.DataFrame({i: [] for i in x.columns})\n",
    "    Y = pd.DataFrame({i: [] for i in y.columns})\n",
    "    ids = list(range(len(x)))\n",
    "    while len(ids) > 0:\n",
    "        random_id = random.choice(ids)\n",
    "        X.loc[len(X)] = x.iloc[random_id,]\n",
    "        Y.loc[len(Y)] = y.iloc[random_id,]\n",
    "        ids.remove(random_id)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = shuffle_data(counts_pd, labels_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUxSCyz7jBQf"
   },
   "source": [
    "Shuffle your data. Make sure your labels and the counts are shuffled together.\n",
    "\n",
    "Split into train and test sets (80:20 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XDTqBhcA7V8t"
   },
   "outputs": [],
   "source": [
    "def train_test_split(x, y, test_percent=0.2):\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "    threshold_number = int(0.2 * len(y))\n",
    "    X_test = x.iloc[:threshold_number, :]\n",
    "    y_test = y.iloc[:threshold_number, :]\n",
    "    X_train = x.iloc[threshold_number:, :]\n",
    "    y_train = y.iloc[threshold_number:, :]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)/(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHIg7i1k7U-G"
   },
   "source": [
    "Create a fully connected neural network for your autoencoder. Your latent space can be of any size less than or equal to 64. Too large may result in a poor visualization, and too small may result in high loss. 32 is a good starting point.\n",
    "\n",
    "Consider using more than 1 hidden layer, and a sparcity constraint (l1 regularization).\n",
    "\n",
    "Have an encoder model which is a model of only the layers for the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "b8mvigLP7Sej"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(in_features=kwargs[\"input_shape\"], out_features=128),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(in_features=128, out_features=64),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(in_features=64, out_features=kwargs[\"latent_space\"]))\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.Linear(in_features=kwargs[\"latent_space\"], out_features=64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(in_features=64, out_features=128),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(in_features=128, out_features=kwargs[\"input_shape\"]))\n",
    "        \n",
    "    def forward(self, features, return_encoding=False):\n",
    "        encoded = self.encoder(features)\n",
    "        decoded = self.decoder(encoded)\n",
    "        if return_encoding:\n",
    "            return decoded, encoded\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestClass():\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def test(self, hyper):\n",
    "        result_to_plot = {\"batch\":[], \"lr\":[], \"latent\":[], \"epochs\":[], \"test_loss\":[]}\n",
    "        criterion = nn.MSELoss()\n",
    "        best_loss = 100000\n",
    "        best_parameters = None\n",
    "        for batch in hyper[\"batch\"]:\n",
    "            train_loader = DataLoader(dataset_train, batch_size=batch, shuffle=True)\n",
    "            test_loader = DataLoader(dataset_test, batch_size=batch, shuffle=True)\n",
    "            for lr in hyper[\"lr\"]:\n",
    "                for ls in hyper[\"latent_space\"]:\n",
    "                    model = Autoencoder(input_shape=765, latent_space=ls)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    for epoch in hyper[\"epochs\"]:\n",
    "                        for ep in range(epoch):\n",
    "                            loss = 0\n",
    "                            for batch_features, _ in train_loader:\n",
    "                                batch_features = batch_features.view(-1, 765).to(self.device)\n",
    "                                optimizer.zero_grad()\n",
    "\n",
    "                                outputs, encoded = model(batch_features, return_encoding = True)\n",
    "                                train_loss = criterion(outputs, batch_features)\n",
    "                                train_loss.backward()\n",
    "                                optimizer.step()\n",
    "                                loss+=train_loss.item()\n",
    "                            loss = loss/len(train_loader)\n",
    "                            \n",
    "\n",
    "                        model.eval()\n",
    "                        total_test_loss = 0\n",
    "                        with torch.no_grad():\n",
    "                            for batch_features, _ in test_loader:\n",
    "                                batch_features = batch_features.view(-1, 765).to(self.device)\n",
    "                                output, _ = model(batch_features, return_encoding=True)\n",
    "                                test_loss = criterion(output, batch_features)\n",
    "                                total_test_loss += test_loss.item()\n",
    "                            total_test_loss = total_test_loss/len(test_loader)\n",
    "                            if total_test_loss < best_loss:\n",
    "                                print(\"Best changed!\")\n",
    "                                best_loss = total_test_loss\n",
    "                                best_parameters = {\"ls\":ls, \"epoch\":epoch, \"batch\":batch, \"lr\":lr}\n",
    "\n",
    "                        print(f\"Latent Dimension: {ls}, Epoch: {epoch}, Batch: {batch}, Lr: {lr}, Test Loss: {total_test_loss}\")\n",
    "                        result_to_plot[\"latent\"].append(ls)\n",
    "                        result_to_plot[\"batch\"].append(batch)\n",
    "                        result_to_plot[\"epochs\"].append(epoch)\n",
    "                        result_to_plot[\"lr\"].append(lr)\n",
    "                        result_to_plot[\"test_loss\"].append(total_test_loss)\n",
    "                        \n",
    "        return result_to_plot, best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=[\"Unnamed: 0\"])\n",
    "X_test = X_test.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train[\"bulk_labels\"])\n",
    "y_test_encoded = le.transform(y_test[\"bulk_labels\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_tensor_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "X_tensor_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_tensor_train = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_tensor_test = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "dataset_train = TensorDataset(X_tensor_train, y_tensor_train)\n",
    "dataset_test = TensorDataset(X_tensor_test, y_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [30, 50, 64, 128]\n",
    "lrs = [1e-5, 1e-4, 1e-3, 0.01]\n",
    "latent_dims = [24, 32, 48, 64]\n",
    "batches = [16, 32, 64, 128]\n",
    "\n",
    "hyperparameters = {\"lr\":lrs,\n",
    "                  \"epochs\": epochs,\n",
    "                  \"latent_space\": latent_dims,\n",
    "                  \"batch\": batches}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Gk1sfdNe76Kl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best changed!\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 16, Lr: 1e-05, Test Loss: 0.9596606824133131\n",
      "Best changed!\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 16, Lr: 1e-05, Test Loss: 0.9026759531762865\n",
      "Best changed!\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 16, Lr: 1e-05, Test Loss: 0.8745915095011393\n",
      "Best changed!\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 16, Lr: 1e-05, Test Loss: 0.8453111516104804\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 16, Lr: 1e-05, Test Loss: 0.965838783317142\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 16, Lr: 1e-05, Test Loss: 0.9033121334181892\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 16, Lr: 1e-05, Test Loss: 0.8670467270745171\n",
      "Best changed!\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 16, Lr: 1e-05, Test Loss: 0.8410376244121127\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 16, Lr: 1e-05, Test Loss: 0.9603442483478122\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 16, Lr: 1e-05, Test Loss: 0.8941016064749824\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 16, Lr: 1e-05, Test Loss: 0.8772902753618028\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 16, Lr: 1e-05, Test Loss: 0.8479443656073676\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 16, Lr: 1e-05, Test Loss: 0.9644066691398621\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 16, Lr: 1e-05, Test Loss: 0.8843871355056763\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 16, Lr: 1e-05, Test Loss: 0.8595329655541314\n",
      "Best changed!\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 16, Lr: 1e-05, Test Loss: 0.8351533015569051\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 16, Lr: 0.0001, Test Loss: 0.8351951307720609\n",
      "Best changed!\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 16, Lr: 0.0001, Test Loss: 0.8200179470909966\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 16, Lr: 0.0001, Test Loss: 0.823280500041114\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 16, Lr: 0.0001, Test Loss: 0.8359440962473551\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 16, Lr: 0.0001, Test Loss: 0.8332012494405111\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 16, Lr: 0.0001, Test Loss: 0.8211019767655267\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 16, Lr: 0.0001, Test Loss: 0.8265903194745382\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 16, Lr: 0.0001, Test Loss: 0.8457614183425903\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 16, Lr: 0.0001, Test Loss: 0.8281131982803345\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 16, Lr: 0.0001, Test Loss: 0.8221292230818007\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 16, Lr: 0.0001, Test Loss: 0.8218931092156304\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 16, Lr: 0.0001, Test Loss: 0.8390934268633524\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 16, Lr: 0.0001, Test Loss: 0.8348349531491598\n",
      "Best changed!\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 16, Lr: 0.0001, Test Loss: 0.81812451283137\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 16, Lr: 0.0001, Test Loss: 0.822778754764133\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 16, Lr: 0.0001, Test Loss: 0.8396067950460646\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 16, Lr: 0.001, Test Loss: 0.8370341195000542\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 16, Lr: 0.001, Test Loss: 0.8758793870608012\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 16, Lr: 0.001, Test Loss: 0.8971618943744235\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 16, Lr: 0.001, Test Loss: 0.911595861117045\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 16, Lr: 0.001, Test Loss: 0.8327345781856113\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 16, Lr: 0.001, Test Loss: 0.8747456669807434\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 16, Lr: 0.001, Test Loss: 0.8897228903240628\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 16, Lr: 0.001, Test Loss: 0.8976032270325555\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 16, Lr: 0.001, Test Loss: 0.8313302199045817\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 16, Lr: 0.001, Test Loss: 0.8697566919856601\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 16, Lr: 0.001, Test Loss: 0.885465059015486\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 16, Lr: 0.001, Test Loss: 0.8926123513115777\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 16, Lr: 0.001, Test Loss: 0.8325023452440897\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 16, Lr: 0.001, Test Loss: 0.8648942179150052\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 16, Lr: 0.001, Test Loss: 0.8768263657887777\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 16, Lr: 0.001, Test Loss: 0.8917414347330729\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 16, Lr: 0.01, Test Loss: 0.8936548630396525\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 16, Lr: 0.01, Test Loss: 0.967573344707489\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 16, Lr: 0.01, Test Loss: 0.9686770968967013\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 16, Lr: 0.01, Test Loss: 0.9657993581559923\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 16, Lr: 0.01, Test Loss: 0.9063984288109673\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 16, Lr: 0.01, Test Loss: 0.9214189847310384\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 16, Lr: 0.01, Test Loss: 0.9395719634162055\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 16, Lr: 0.01, Test Loss: 0.930835399362776\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 16, Lr: 0.01, Test Loss: 0.9641160302691989\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 16, Lr: 0.01, Test Loss: 0.9400513238377042\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 16, Lr: 0.01, Test Loss: 0.9419195519553291\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 16, Lr: 0.01, Test Loss: 0.9385784268379211\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 16, Lr: 0.01, Test Loss: 0.940355294280582\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 16, Lr: 0.01, Test Loss: 0.9388313624593947\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 16, Lr: 0.01, Test Loss: 0.965603596634335\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 16, Lr: 0.01, Test Loss: 0.9655518068207635\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 32, Lr: 1e-05, Test Loss: 0.959345531463623\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 32, Lr: 1e-05, Test Loss: 0.9408127307891846\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 32, Lr: 1e-05, Test Loss: 0.8854697108268738\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 32, Lr: 1e-05, Test Loss: 0.854265546798706\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 32, Lr: 1e-05, Test Loss: 0.9685033917427063\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 32, Lr: 1e-05, Test Loss: 0.9243856191635131\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 32, Lr: 1e-05, Test Loss: 0.8912712097167969\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 32, Lr: 1e-05, Test Loss: 0.8609833955764771\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 32, Lr: 1e-05, Test Loss: 0.9724983811378479\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 32, Lr: 1e-05, Test Loss: 0.930020523071289\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 32, Lr: 1e-05, Test Loss: 0.8787919163703919\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 32, Lr: 1e-05, Test Loss: 0.8613375186920166\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 32, Lr: 1e-05, Test Loss: 0.9635036945343017\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 32, Lr: 1e-05, Test Loss: 0.9444532990455627\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 32, Lr: 1e-05, Test Loss: 0.8774878144264221\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 32, Lr: 1e-05, Test Loss: 0.8581352472305298\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 32, Lr: 0.0001, Test Loss: 0.8568857908248901\n",
      "Best changed!\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 32, Lr: 0.0001, Test Loss: 0.8096693396568299\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 32, Lr: 0.0001, Test Loss: 0.8356997847557068\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 32, Lr: 0.0001, Test Loss: 0.8288063287734986\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 32, Lr: 0.0001, Test Loss: 0.8456904292106628\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 32, Lr: 0.0001, Test Loss: 0.8192451119422912\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 32, Lr: 0.0001, Test Loss: 0.8222010016441346\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 32, Lr: 0.0001, Test Loss: 0.8172568082809448\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 32, Lr: 0.0001, Test Loss: 0.8322866678237915\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 32, Lr: 0.0001, Test Loss: 0.8170918822288513\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 32, Lr: 0.0001, Test Loss: 0.8214041233062744\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 32, Lr: 0.0001, Test Loss: 0.8326847791671753\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 32, Lr: 0.0001, Test Loss: 0.8490789532661438\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 32, Lr: 0.0001, Test Loss: 0.8251301527023316\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 32, Lr: 0.0001, Test Loss: 0.8227270722389222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Dimension: 64, Epoch: 128, Batch: 32, Lr: 0.0001, Test Loss: 0.8392776608467102\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 32, Lr: 0.001, Test Loss: 0.8229466319084168\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 32, Lr: 0.001, Test Loss: 0.8707501769065857\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 32, Lr: 0.001, Test Loss: 0.9075612306594849\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 32, Lr: 0.001, Test Loss: 0.929765522480011\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 32, Lr: 0.001, Test Loss: 0.8509032487869262\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 32, Lr: 0.001, Test Loss: 0.8798050999641418\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 32, Lr: 0.001, Test Loss: 0.8916779518127441\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 32, Lr: 0.001, Test Loss: 0.9118680238723755\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 32, Lr: 0.001, Test Loss: 0.828657352924347\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 32, Lr: 0.001, Test Loss: 0.8767669916152954\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 32, Lr: 0.001, Test Loss: 0.8937505006790161\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 32, Lr: 0.001, Test Loss: 0.9173540949821473\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 32, Lr: 0.001, Test Loss: 0.8116153120994568\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 32, Lr: 0.001, Test Loss: 0.8710526347160339\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 32, Lr: 0.001, Test Loss: 0.8948094844818115\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 32, Lr: 0.001, Test Loss: 0.9082988739013672\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 32, Lr: 0.01, Test Loss: 0.8682717084884644\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 32, Lr: 0.01, Test Loss: 0.9415847778320312\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 32, Lr: 0.01, Test Loss: 0.9367865204811097\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 32, Lr: 0.01, Test Loss: 0.9243749260902405\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 32, Lr: 0.01, Test Loss: 0.8673689246177674\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 32, Lr: 0.01, Test Loss: 0.8554653763771057\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 32, Lr: 0.01, Test Loss: 0.8594278812408447\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 32, Lr: 0.01, Test Loss: 0.9229353785514831\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 32, Lr: 0.01, Test Loss: 0.8760366916656495\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 32, Lr: 0.01, Test Loss: 0.8681482911109925\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 32, Lr: 0.01, Test Loss: 0.9559056758880615\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 32, Lr: 0.01, Test Loss: 0.9316242814064026\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 32, Lr: 0.01, Test Loss: 0.8526856899261475\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 32, Lr: 0.01, Test Loss: 0.8904284238815308\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 32, Lr: 0.01, Test Loss: 0.9552771329879761\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 32, Lr: 0.01, Test Loss: 0.9453021168708802\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 64, Lr: 1e-05, Test Loss: 0.9518706997235616\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 64, Lr: 1e-05, Test Loss: 0.949580192565918\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 64, Lr: 1e-05, Test Loss: 0.9213144779205322\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 64, Lr: 1e-05, Test Loss: 0.861391286055247\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 64, Lr: 1e-05, Test Loss: 0.9755652149518331\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 64, Lr: 1e-05, Test Loss: 0.972037156422933\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 64, Lr: 1e-05, Test Loss: 0.9189509550730387\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 64, Lr: 1e-05, Test Loss: 0.8695122599601746\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 64, Lr: 1e-05, Test Loss: 0.9502355257670084\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 64, Lr: 1e-05, Test Loss: 1.0030159155527751\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 64, Lr: 1e-05, Test Loss: 0.95531165599823\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 64, Lr: 1e-05, Test Loss: 0.8742152253786722\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 64, Lr: 1e-05, Test Loss: 0.9596586426099142\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 64, Lr: 1e-05, Test Loss: 0.9588915506998698\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 64, Lr: 1e-05, Test Loss: 0.9172096252441406\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 64, Lr: 1e-05, Test Loss: 0.8693937460581461\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 64, Lr: 0.0001, Test Loss: 0.8997275630633036\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 64, Lr: 0.0001, Test Loss: 0.8355466922124227\n",
      "Best changed!\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 64, Lr: 0.0001, Test Loss: 0.7998818953831991\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 64, Lr: 0.0001, Test Loss: 0.8504577080408732\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 64, Lr: 0.0001, Test Loss: 0.8614641229311625\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 64, Lr: 0.0001, Test Loss: 0.8270416855812073\n",
      "Best changed!\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 64, Lr: 0.0001, Test Loss: 0.7980700135231018\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 64, Lr: 0.0001, Test Loss: 0.8201746940612793\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 64, Lr: 0.0001, Test Loss: 0.8853234052658081\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 64, Lr: 0.0001, Test Loss: 0.8247132698694865\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 64, Lr: 0.0001, Test Loss: 0.8560588161150614\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 64, Lr: 0.0001, Test Loss: 0.7992529074350992\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 64, Lr: 0.0001, Test Loss: 0.8873247305552164\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 64, Lr: 0.0001, Test Loss: 0.8365976015726725\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 64, Lr: 0.0001, Test Loss: 0.8108812570571899\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 64, Lr: 0.0001, Test Loss: 0.8688995440800985\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 64, Lr: 0.001, Test Loss: 0.8372346758842468\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 64, Lr: 0.001, Test Loss: 0.9003211855888367\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 64, Lr: 0.001, Test Loss: 0.8688373764355978\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 64, Lr: 0.001, Test Loss: 0.9328398505846659\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 64, Lr: 0.001, Test Loss: 0.8479567766189575\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 64, Lr: 0.001, Test Loss: 0.8630456725756327\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 64, Lr: 0.001, Test Loss: 0.8793107271194458\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 64, Lr: 0.001, Test Loss: 0.9337556759516398\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 64, Lr: 0.001, Test Loss: 0.8072788516680399\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 64, Lr: 0.001, Test Loss: 0.8486377000808716\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 64, Lr: 0.001, Test Loss: 0.8817742864290873\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 64, Lr: 0.001, Test Loss: 0.9160180489222208\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 64, Lr: 0.001, Test Loss: 0.8203224937121073\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 64, Lr: 0.001, Test Loss: 0.8487758239110311\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 64, Lr: 0.001, Test Loss: 0.8963588078816732\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 64, Lr: 0.001, Test Loss: 0.9101520578066508\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 64, Lr: 0.01, Test Loss: 0.8421865105628967\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 64, Lr: 0.01, Test Loss: 0.8699559966723124\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 64, Lr: 0.01, Test Loss: 0.8756837248802185\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 64, Lr: 0.01, Test Loss: 0.8404586315155029\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 64, Lr: 0.01, Test Loss: 0.8434605399767557\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 64, Lr: 0.01, Test Loss: 0.8324963053067526\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 64, Lr: 0.01, Test Loss: 0.8339157700538635\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 64, Lr: 0.01, Test Loss: 0.8602136373519897\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 64, Lr: 0.01, Test Loss: 0.8303296764691671\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 64, Lr: 0.01, Test Loss: 0.8634218573570251\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 64, Lr: 0.01, Test Loss: 0.8205792109171549\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 64, Lr: 0.01, Test Loss: 0.9347058137257894\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 64, Lr: 0.01, Test Loss: 0.8638389309247335\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 64, Lr: 0.01, Test Loss: 0.8353811899820963\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 64, Lr: 0.01, Test Loss: 0.8419569134712219\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 64, Lr: 0.01, Test Loss: 0.9040742119153341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Dimension: 24, Epoch: 30, Batch: 128, Lr: 1e-05, Test Loss: 0.902366042137146\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 128, Lr: 1e-05, Test Loss: 0.9073465764522552\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 128, Lr: 1e-05, Test Loss: 0.9683083891868591\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 128, Lr: 1e-05, Test Loss: 0.9713256061077118\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 128, Lr: 1e-05, Test Loss: 1.000637173652649\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 128, Lr: 1e-05, Test Loss: 0.9308198094367981\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 128, Lr: 1e-05, Test Loss: 0.9871404767036438\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 128, Lr: 1e-05, Test Loss: 0.9044370651245117\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 128, Lr: 1e-05, Test Loss: 0.9738779962062836\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 128, Lr: 1e-05, Test Loss: 0.9401305615901947\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 128, Lr: 1e-05, Test Loss: 0.9450983703136444\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 128, Lr: 1e-05, Test Loss: 0.9719688594341278\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 128, Lr: 1e-05, Test Loss: 0.928816556930542\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 128, Lr: 1e-05, Test Loss: 0.8740945756435394\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 128, Lr: 1e-05, Test Loss: 0.9222598671913147\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 128, Lr: 1e-05, Test Loss: 0.9219868779182434\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 128, Lr: 0.0001, Test Loss: 0.8864936530590057\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 128, Lr: 0.0001, Test Loss: 0.8962476253509521\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 128, Lr: 0.0001, Test Loss: 0.802204042673111\n",
      "Best changed!\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 128, Lr: 0.0001, Test Loss: 0.7941685318946838\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 128, Lr: 0.0001, Test Loss: 0.9312559366226196\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 128, Lr: 0.0001, Test Loss: 0.8129266202449799\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 128, Lr: 0.0001, Test Loss: 0.8522006571292877\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 128, Lr: 0.0001, Test Loss: 0.8063177168369293\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 128, Lr: 0.0001, Test Loss: 0.8721084892749786\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 128, Lr: 0.0001, Test Loss: 0.8483884930610657\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 128, Lr: 0.0001, Test Loss: 0.8259314596652985\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 128, Lr: 0.0001, Test Loss: 0.8613011538982391\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 128, Lr: 0.0001, Test Loss: 0.9546833038330078\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 128, Lr: 0.0001, Test Loss: 0.8449345529079437\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 128, Lr: 0.0001, Test Loss: 0.848654717206955\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 128, Lr: 0.0001, Test Loss: 0.829244464635849\n",
      "Best changed!\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 128, Lr: 0.001, Test Loss: 0.7752377092838287\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 128, Lr: 0.001, Test Loss: 0.8650354444980621\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 128, Lr: 0.001, Test Loss: 0.8708830177783966\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 128, Lr: 0.001, Test Loss: 0.9640872478485107\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 128, Lr: 0.001, Test Loss: 0.7755520641803741\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 128, Lr: 0.001, Test Loss: 0.8001337051391602\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 128, Lr: 0.001, Test Loss: 0.8508781492710114\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 128, Lr: 0.001, Test Loss: 0.9044471085071564\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 128, Lr: 0.001, Test Loss: 0.8694703876972198\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 128, Lr: 0.001, Test Loss: 0.8147282004356384\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 128, Lr: 0.001, Test Loss: 0.8958373665809631\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 128, Lr: 0.001, Test Loss: 0.8649440705776215\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 128, Lr: 0.001, Test Loss: 0.8484501838684082\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 128, Lr: 0.001, Test Loss: 0.8237084150314331\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 128, Lr: 0.001, Test Loss: 0.8311437964439392\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 128, Lr: 0.001, Test Loss: 0.9108383655548096\n",
      "Latent Dimension: 24, Epoch: 30, Batch: 128, Lr: 0.01, Test Loss: 0.8315929174423218\n",
      "Latent Dimension: 24, Epoch: 50, Batch: 128, Lr: 0.01, Test Loss: 0.8728789389133453\n",
      "Latent Dimension: 24, Epoch: 64, Batch: 128, Lr: 0.01, Test Loss: 0.8967787027359009\n",
      "Latent Dimension: 24, Epoch: 128, Batch: 128, Lr: 0.01, Test Loss: 0.8399766683578491\n",
      "Latent Dimension: 32, Epoch: 30, Batch: 128, Lr: 0.01, Test Loss: 0.8227249681949615\n",
      "Latent Dimension: 32, Epoch: 50, Batch: 128, Lr: 0.01, Test Loss: 0.8421827256679535\n",
      "Latent Dimension: 32, Epoch: 64, Batch: 128, Lr: 0.01, Test Loss: 0.8740655481815338\n",
      "Latent Dimension: 32, Epoch: 128, Batch: 128, Lr: 0.01, Test Loss: 0.9393362998962402\n",
      "Latent Dimension: 48, Epoch: 30, Batch: 128, Lr: 0.01, Test Loss: 0.8848279416561127\n",
      "Latent Dimension: 48, Epoch: 50, Batch: 128, Lr: 0.01, Test Loss: 0.8746422529220581\n",
      "Latent Dimension: 48, Epoch: 64, Batch: 128, Lr: 0.01, Test Loss: 0.832025408744812\n",
      "Latent Dimension: 48, Epoch: 128, Batch: 128, Lr: 0.01, Test Loss: 0.91652050614357\n",
      "Latent Dimension: 64, Epoch: 30, Batch: 128, Lr: 0.01, Test Loss: 0.8354625105857849\n",
      "Latent Dimension: 64, Epoch: 50, Batch: 128, Lr: 0.01, Test Loss: 0.8529891073703766\n",
      "Latent Dimension: 64, Epoch: 64, Batch: 128, Lr: 0.01, Test Loss: 0.8409555852413177\n",
      "Latent Dimension: 64, Epoch: 128, Batch: 128, Lr: 0.01, Test Loss: 0.8856329023838043\n"
     ]
    }
   ],
   "source": [
    "tc = TestClass()\n",
    "result_to_plot, best_parameters = tc.test(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result_to_plot)\n",
    "df.to_csv(\"test_training_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch</th>\n",
       "      <th>lr</th>\n",
       "      <th>latent</th>\n",
       "      <th>epochs</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>0.959661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>0.902676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>24</td>\n",
       "      <td>64</td>\n",
       "      <td>0.874592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>24</td>\n",
       "      <td>128</td>\n",
       "      <td>0.845311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>0.965839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>128</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>48</td>\n",
       "      <td>128</td>\n",
       "      <td>0.916521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>128</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>0.835463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>128</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "      <td>0.852989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>128</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0.840956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>128</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>0.885633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     batch       lr  latent  epochs  test_loss\n",
       "0       16  0.00001      24      30   0.959661\n",
       "1       16  0.00001      24      50   0.902676\n",
       "2       16  0.00001      24      64   0.874592\n",
       "3       16  0.00001      24     128   0.845311\n",
       "4       16  0.00001      32      30   0.965839\n",
       "..     ...      ...     ...     ...        ...\n",
       "251    128  0.01000      48     128   0.916521\n",
       "252    128  0.01000      64      30   0.835463\n",
       "253    128  0.01000      64      50   0.852989\n",
       "254    128  0.01000      64      64   0.840956\n",
       "255    128  0.01000      64     128   0.885633\n",
       "\n",
       "[256 rows x 5 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-50-1321d7cd36cf>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-50-1321d7cd36cf>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    df_new = df.groupby([parameter]).agg(\"test_lost\":\"mean\")\u001b[0m\n\u001b[1;37m                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_comparison(df, parameter):\n",
    "    df_new = df.groupby([parameter]).agg(\"test_lost\":\"mean\")\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    print(df_new.columns)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['test_lost'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-7a54d965018b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_comparison\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"latent\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-c1d0d8b1827f>\u001b[0m in \u001b[0;36mget_comparison\u001b[1;34m(df, parameter)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_comparison\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdf_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparameter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"test_lost\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cagin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    980\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cagin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;31m# we require a list, but not a 'str'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cagin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36magg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[0mselection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m         \u001b[0marg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_dictlike_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"agg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cagin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mnormalize_dictlike_arg\u001b[1;34m(self, how, obj, func)\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m                 \u001b[0mcols_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Column(s) {cols_sorted} do not exist\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[0mis_aggregator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Column(s) ['test_lost'] do not exist\""
     ]
    }
   ],
   "source": [
    "get_comparison(df, \"latent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjQr4OYW76bN"
   },
   "source": [
    "Train your autoencoding using MSE loss.\n",
    "\n",
    "Finally, identify the parameters which don't overfit, and use the same model architecture and train on all of the data together.\n",
    "\n",
    "With a latent space size of 32, aim for 0.9 MSE loss on your test set, 0.95 with regularization. You will not be graded strictly on a loss cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4Q6KU3c8u-E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Trjfxkk8wyg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yweQRGit8xDX"
   },
   "source": [
    "Use PCA and t-SNE on the dataset.\n",
    "\n",
    "Then use PCA on the latent space representation of the dataset.\n",
    "\n",
    "Plot all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGa5B6Ir9KN4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2xcMPP09KxV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfOYsI9S9K5M"
   },
   "source": [
    "Compare the results of PCA, t-SNE, and your autoencoder as ways to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DPmGoHo9uwx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Final Project Part 1 - Autoencoder",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
